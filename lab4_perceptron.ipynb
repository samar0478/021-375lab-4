{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfKtPBnmc7ARNmEt9HbAE4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samar0478/021-375lab-4/blob/main/lab4_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-dZRzfJIlFSo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "perceptron for  2 input basic gates"
      ],
      "metadata": {
        "id": "5qJEq5SSmg5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linearInt(x, weights, bias):\n",
        "    return sum([x[i] * weights[i] for i in range(len(x))]) + bias\n",
        "\n",
        "def predict(x, weights, bias):\n",
        "    return 1 if linearInt(x, weights, bias) >= 0 else 0\n",
        "\n",
        "def train_perceptron(X, Y, learning_rate=0.1, max_iterations=100):\n",
        "    weights = [0, 0]\n",
        "    bias = 0\n",
        "\n",
        "    for _ in range(max_iterations):  # max_iterations to avoid infinite loop\n",
        "        error_found = False\n",
        "        for i in range(len(X)):\n",
        "            x = X[i]\n",
        "            y = Y[i]\n",
        "            y_pred = predict(x, weights, bias)\n",
        "            error = y - y_pred\n",
        "\n",
        "            if error != 0:\n",
        "                # Update weights and bias\n",
        "                for j in range(len(weights)):\n",
        "                    weights[j] += learning_rate * error * x[j]\n",
        "                bias += learning_rate * error\n",
        "                error_found = True\n",
        "\n",
        "        if not error_found:\n",
        "            break  # Stop if no errors (converged)\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def accuracy(X, Y, weights, bias):\n",
        "    correct = 0\n",
        "    for i in range(len(X)):\n",
        "        if predict(X[i], weights, bias) == Y[i]:\n",
        "            correct += 1\n",
        "    return (correct / len(X)) * 100\n",
        "\n",
        "# Input truth table\n",
        "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "\n",
        "# AND Gate\n",
        "print(\"---- AND Gate ----\")\n",
        "Y_and = [0, 0, 0, 1]\n",
        "w_and, b_and = train_perceptron(X, Y_and)\n",
        "print(f\"Weights: {w_and}, Bias: {b_and}\")\n",
        "print(f\"Accuracy: {accuracy(X, Y_and, w_and, b_and)}%\")\n",
        "\n",
        "# OR Gate\n",
        "print(\"\\n---- OR Gate ----\")\n",
        "Y_or = [0, 1, 1, 1]\n",
        "w_or, b_or = train_perceptron(X, Y_or)\n",
        "print(f\"Weights: {w_or}, Bias: {b_or}\")\n",
        "print(f\"Accuracy: {accuracy(X, Y_or, w_or, b_or)}%\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUBZWrp4mDNN",
        "outputId": "169700bc-b0c8-499e-9558-49e481fea2d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- AND Gate ----\n",
            "Weights: [0.2, 0.1], Bias: -0.20000000000000004\n",
            "Accuracy: 100.0%\n",
            "\n",
            "---- OR Gate ----\n",
            "Weights: [0.1, 0.1], Bias: -0.1\n",
            "Accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oimmadxymyZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "perceptron for n input basic gates\n"
      ],
      "metadata": {
        "id": "Xt1PSdB8mtB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "def linearInt(x, weights, bias):\n",
        "    return sum([x[i] * weights[i] for i in range(len(x))]) + bias\n",
        "\n",
        "def predict(x, weights, bias):\n",
        "    return 1 if linearInt(x, weights, bias) >= 0 else 0\n",
        "\n",
        "def train(X, Y, lr=0.1, max_loops=100):\n",
        "    weights = [0.0] * len(X[0])\n",
        "    bias = 0.0\n",
        "\n",
        "    for _ in range(max_loops):\n",
        "        error_found = False\n",
        "        for i in range(len(X)):\n",
        "            y_pred = predict(X[i], weights, bias)\n",
        "            error = Y[i] - y_pred\n",
        "            if error != 0:\n",
        "                for j in range(len(weights)):\n",
        "                    weights[j] += lr * error * X[i][j]\n",
        "                bias += lr * error\n",
        "                error_found = True\n",
        "        if not error_found:\n",
        "            break\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def test_accuracy(X, Y, weights, bias):\n",
        "    correct = 0\n",
        "    for i in range(len(X)):\n",
        "        if predict(X[i], weights, bias) == Y[i]:\n",
        "            correct += 1\n",
        "    return (correct / len(X)) * 100\n",
        "\n",
        "# Run for both 3-input and 4-input gates\n",
        "for n in [3, 4]:\n",
        "    print(f\"\\n==== {n}-INPUT GATES ====\")\n",
        "    X = list(product([0, 1], repeat=n))\n",
        "\n",
        "    # AND Gate\n",
        "    Y_and = [int(all(x)) for x in X]\n",
        "    w_and, b_and = train(X, Y_and)\n",
        "    print(f\"\\nAND Gate:\")\n",
        "    print(f\"Weights: {w_and}\")\n",
        "    print(f\"Bias: {b_and}\")\n",
        "    print(f\"Accuracy: {test_accuracy(X, Y_and, w_and, b_and)}%\")\n",
        "\n",
        "    # OR Gate\n",
        "    Y_or = [int(any(x)) for x in X]\n",
        "    w_or, b_or = train(X, Y_or)\n",
        "    print(f\"\\nOR Gate:\")\n",
        "    print(f\"Weights: {w_or}\")\n",
        "    print(f\"Bias: {b_or}\")\n",
        "    print(f\"Accuracy: {test_accuracy(X, Y_or, w_or, b_or)}%\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q2MdSnbmzNs",
        "outputId": "ff752922-3f05-443f-a58f-f6e7dfc2fdc5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== 3-INPUT GATES ====\n",
            "\n",
            "AND Gate:\n",
            "Weights: [0.1, 0.1, 0.1]\n",
            "Bias: -0.20000000000000004\n",
            "Accuracy: 100.0%\n",
            "\n",
            "OR Gate:\n",
            "Weights: [0.1, 0.1, 0.1]\n",
            "Bias: -0.1\n",
            "Accuracy: 100.0%\n",
            "\n",
            "==== 4-INPUT GATES ====\n",
            "\n",
            "AND Gate:\n",
            "Weights: [0.4, 0.20000000000000004, 0.1, 0.1]\n",
            "Bias: -0.7999999999999999\n",
            "Accuracy: 100.0%\n",
            "\n",
            "OR Gate:\n",
            "Weights: [0.1, 0.1, 0.1, 0.1]\n",
            "Bias: -0.1\n",
            "Accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron for linear function with 3 features"
      ],
      "metadata": {
        "id": "AOcHHWAfm7rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Generate dataset: y = 2x1 + 3x2 - x3 + 5\n",
        "X = []\n",
        "Y = []\n",
        "for _ in range(10):\n",
        "    x1 = random.uniform(0, 1)\n",
        "    x2 = random.uniform(0, 1)\n",
        "    x3 = random.uniform(0, 1)\n",
        "    y = 2 * x1 + 3 * x2 - 1 * x3 + 5\n",
        "    X.append([x1, x2, x3])\n",
        "    Y.append(y)\n",
        "\n",
        "# Initialize weights and bias\n",
        "weights = [0.0, 0.0, 0.0]\n",
        "bias = 0.0\n",
        "lr = 0.01\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    total_error = 0\n",
        "    for i in range(len(X)):\n",
        "        x = X[i]\n",
        "        y_true = Y[i]\n",
        "\n",
        "        # Linear output (no activation)\n",
        "        y_pred = sum([x[j] * weights[j] for j in range(3)]) + bias\n",
        "\n",
        "        # Error\n",
        "        error = y_true - y_pred\n",
        "\n",
        "        # Update weights and bias\n",
        "        for j in range(3):\n",
        "            weights[j] += lr * error * x[j]\n",
        "        bias += lr * error\n",
        "\n",
        "        total_error += error ** 2  # squared error\n",
        "\n",
        "    mse = total_error / len(X)\n",
        "    print(f\"Epoch {epoch+1}: MSE = {mse:.4f}\")\n",
        "\n",
        "# Final result\n",
        "print(\"\\nFinal Weights and Bias:\")\n",
        "print(f\"Weights: {weights}\")\n",
        "print(f\"Bias: {bias}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU6ypRKsnIJ1",
        "outputId": "34e7f896-75b6-406c-a8ec-756e20c5b75c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: MSE = 37.3694\n",
            "Epoch 2: MSE = 27.4323\n",
            "Epoch 3: MSE = 20.1756\n",
            "Epoch 4: MSE = 14.8761\n",
            "Epoch 5: MSE = 11.0054\n",
            "Epoch 6: MSE = 8.1779\n",
            "Epoch 7: MSE = 6.1121\n",
            "Epoch 8: MSE = 4.6023\n",
            "Epoch 9: MSE = 3.4986\n",
            "Epoch 10: MSE = 2.6912\n",
            "Epoch 11: MSE = 2.1003\n",
            "Epoch 12: MSE = 1.6673\n",
            "Epoch 13: MSE = 1.3497\n",
            "Epoch 14: MSE = 1.1163\n",
            "Epoch 15: MSE = 0.9444\n",
            "Epoch 16: MSE = 0.8174\n",
            "Epoch 17: MSE = 0.7232\n",
            "Epoch 18: MSE = 0.6530\n",
            "Epoch 19: MSE = 0.6002\n",
            "Epoch 20: MSE = 0.5602\n",
            "Epoch 21: MSE = 0.5296\n",
            "Epoch 22: MSE = 0.5059\n",
            "Epoch 23: MSE = 0.4871\n",
            "Epoch 24: MSE = 0.4720\n",
            "Epoch 25: MSE = 0.4597\n",
            "Epoch 26: MSE = 0.4493\n",
            "Epoch 27: MSE = 0.4403\n",
            "Epoch 28: MSE = 0.4325\n",
            "Epoch 29: MSE = 0.4255\n",
            "Epoch 30: MSE = 0.4191\n",
            "Epoch 31: MSE = 0.4132\n",
            "Epoch 32: MSE = 0.4076\n",
            "Epoch 33: MSE = 0.4023\n",
            "Epoch 34: MSE = 0.3972\n",
            "Epoch 35: MSE = 0.3923\n",
            "Epoch 36: MSE = 0.3875\n",
            "Epoch 37: MSE = 0.3829\n",
            "Epoch 38: MSE = 0.3784\n",
            "Epoch 39: MSE = 0.3740\n",
            "Epoch 40: MSE = 0.3696\n",
            "Epoch 41: MSE = 0.3654\n",
            "Epoch 42: MSE = 0.3612\n",
            "Epoch 43: MSE = 0.3571\n",
            "Epoch 44: MSE = 0.3530\n",
            "Epoch 45: MSE = 0.3490\n",
            "Epoch 46: MSE = 0.3451\n",
            "Epoch 47: MSE = 0.3412\n",
            "Epoch 48: MSE = 0.3374\n",
            "Epoch 49: MSE = 0.3336\n",
            "Epoch 50: MSE = 0.3299\n",
            "Epoch 51: MSE = 0.3263\n",
            "Epoch 52: MSE = 0.3227\n",
            "Epoch 53: MSE = 0.3192\n",
            "Epoch 54: MSE = 0.3157\n",
            "Epoch 55: MSE = 0.3122\n",
            "Epoch 56: MSE = 0.3088\n",
            "Epoch 57: MSE = 0.3055\n",
            "Epoch 58: MSE = 0.3022\n",
            "Epoch 59: MSE = 0.2989\n",
            "Epoch 60: MSE = 0.2957\n",
            "Epoch 61: MSE = 0.2926\n",
            "Epoch 62: MSE = 0.2894\n",
            "Epoch 63: MSE = 0.2864\n",
            "Epoch 64: MSE = 0.2833\n",
            "Epoch 65: MSE = 0.2803\n",
            "Epoch 66: MSE = 0.2774\n",
            "Epoch 67: MSE = 0.2745\n",
            "Epoch 68: MSE = 0.2716\n",
            "Epoch 69: MSE = 0.2688\n",
            "Epoch 70: MSE = 0.2660\n",
            "Epoch 71: MSE = 0.2632\n",
            "Epoch 72: MSE = 0.2605\n",
            "Epoch 73: MSE = 0.2578\n",
            "Epoch 74: MSE = 0.2552\n",
            "Epoch 75: MSE = 0.2526\n",
            "Epoch 76: MSE = 0.2500\n",
            "Epoch 77: MSE = 0.2475\n",
            "Epoch 78: MSE = 0.2450\n",
            "Epoch 79: MSE = 0.2425\n",
            "Epoch 80: MSE = 0.2400\n",
            "Epoch 81: MSE = 0.2376\n",
            "Epoch 82: MSE = 0.2352\n",
            "Epoch 83: MSE = 0.2329\n",
            "Epoch 84: MSE = 0.2306\n",
            "Epoch 85: MSE = 0.2283\n",
            "Epoch 86: MSE = 0.2260\n",
            "Epoch 87: MSE = 0.2238\n",
            "Epoch 88: MSE = 0.2216\n",
            "Epoch 89: MSE = 0.2194\n",
            "Epoch 90: MSE = 0.2173\n",
            "Epoch 91: MSE = 0.2151\n",
            "Epoch 92: MSE = 0.2130\n",
            "Epoch 93: MSE = 0.2110\n",
            "Epoch 94: MSE = 0.2089\n",
            "Epoch 95: MSE = 0.2069\n",
            "Epoch 96: MSE = 0.2049\n",
            "Epoch 97: MSE = 0.2030\n",
            "Epoch 98: MSE = 0.2010\n",
            "Epoch 99: MSE = 0.1991\n",
            "Epoch 100: MSE = 0.1972\n",
            "\n",
            "Final Weights and Bias:\n",
            "Weights: [1.7435920261901312, 2.4742300252643536, 1.0626443625457365]\n",
            "Bias: 4.319103546328081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron for linear function with n features"
      ],
      "metadata": {
        "id": "GCF-vmy6nN6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def train_perceptron_linear(X, Y, lr=0.01, epochs=100):\n",
        "    n_features = len(X[0])\n",
        "    weights = [0.0] * n_features\n",
        "    bias = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(X)):\n",
        "            y_pred = sum(X[i][j] * weights[j] for j in range(n_features)) + bias\n",
        "            error = Y[i] - y_pred\n",
        "            # Update weights and bias\n",
        "            for j in range(n_features):\n",
        "                weights[j] += lr * error * X[i][j]\n",
        "            bias += lr * error\n",
        "            total_error += error ** 2\n",
        "        mse = total_error / len(X)\n",
        "        print(f\"Epoch {epoch+1}: MSE = {mse:.4f}\")\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "\n",
        "def generate_dataset(n_features, n_samples=10):\n",
        "    # Generate true random weights in [-1,1]\n",
        "    true_weights = [random.uniform(-1, 1) for _ in range(n_features)]\n",
        "    true_bias = 5  # fixed bias\n",
        "\n",
        "    X = []\n",
        "    Y = []\n",
        "    for _ in range(n_samples):\n",
        "        features = [random.uniform(0, 1) for _ in range(n_features)]\n",
        "        y = sum(features[i] * true_weights[i] for i in range(n_features)) + true_bias\n",
        "        X.append(features)\n",
        "        Y.append(y)\n",
        "\n",
        "    return X, Y, true_weights, true_bias\n",
        "\n",
        "\n",
        "# Test for n=4 and n=5\n",
        "for n in [4, 5]:\n",
        "    print(f\"\\nTraining for n={n} features:\")\n",
        "    X, Y, true_w, true_b = generate_dataset(n)\n",
        "    print(f\"True weights: {true_w}\")\n",
        "    print(f\"True bias: {true_b}\")\n",
        "    learned_weights, learned_bias = train_perceptron_linear(X, Y)\n",
        "    print(f\"Learned weights: {learned_weights}\")\n",
        "    print(f\"Learned bias: {learned_bias}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yks9jwyfnQwE",
        "outputId": "85e4dce4-ca35-4083-86a3-f8d563796a18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training for n=4 features:\n",
            "True weights: [-0.5571996997331994, -0.8837146885107474, 0.947312593108127, 0.7729762673928915]\n",
            "True bias: 5\n",
            "Epoch 1: MSE = 23.5794\n",
            "Epoch 2: MSE = 15.9933\n",
            "Epoch 3: MSE = 10.9153\n",
            "Epoch 4: MSE = 7.5155\n",
            "Epoch 5: MSE = 5.2385\n",
            "Epoch 6: MSE = 3.7127\n",
            "Epoch 7: MSE = 2.6895\n",
            "Epoch 8: MSE = 2.0025\n",
            "Epoch 9: MSE = 1.5405\n",
            "Epoch 10: MSE = 1.2291\n",
            "Epoch 11: MSE = 1.0184\n",
            "Epoch 12: MSE = 0.8751\n",
            "Epoch 13: MSE = 0.7769\n",
            "Epoch 14: MSE = 0.7089\n",
            "Epoch 15: MSE = 0.6612\n",
            "Epoch 16: MSE = 0.6270\n",
            "Epoch 17: MSE = 0.6018\n",
            "Epoch 18: MSE = 0.5828\n",
            "Epoch 19: MSE = 0.5679\n",
            "Epoch 20: MSE = 0.5557\n",
            "Epoch 21: MSE = 0.5455\n",
            "Epoch 22: MSE = 0.5365\n",
            "Epoch 23: MSE = 0.5284\n",
            "Epoch 24: MSE = 0.5209\n",
            "Epoch 25: MSE = 0.5138\n",
            "Epoch 26: MSE = 0.5071\n",
            "Epoch 27: MSE = 0.5006\n",
            "Epoch 28: MSE = 0.4942\n",
            "Epoch 29: MSE = 0.4881\n",
            "Epoch 30: MSE = 0.4820\n",
            "Epoch 31: MSE = 0.4760\n",
            "Epoch 32: MSE = 0.4702\n",
            "Epoch 33: MSE = 0.4644\n",
            "Epoch 34: MSE = 0.4587\n",
            "Epoch 35: MSE = 0.4531\n",
            "Epoch 36: MSE = 0.4476\n",
            "Epoch 37: MSE = 0.4421\n",
            "Epoch 38: MSE = 0.4367\n",
            "Epoch 39: MSE = 0.4314\n",
            "Epoch 40: MSE = 0.4261\n",
            "Epoch 41: MSE = 0.4209\n",
            "Epoch 42: MSE = 0.4158\n",
            "Epoch 43: MSE = 0.4107\n",
            "Epoch 44: MSE = 0.4057\n",
            "Epoch 45: MSE = 0.4008\n",
            "Epoch 46: MSE = 0.3959\n",
            "Epoch 47: MSE = 0.3911\n",
            "Epoch 48: MSE = 0.3864\n",
            "Epoch 49: MSE = 0.3817\n",
            "Epoch 50: MSE = 0.3771\n",
            "Epoch 51: MSE = 0.3725\n",
            "Epoch 52: MSE = 0.3680\n",
            "Epoch 53: MSE = 0.3635\n",
            "Epoch 54: MSE = 0.3591\n",
            "Epoch 55: MSE = 0.3548\n",
            "Epoch 56: MSE = 0.3505\n",
            "Epoch 57: MSE = 0.3463\n",
            "Epoch 58: MSE = 0.3421\n",
            "Epoch 59: MSE = 0.3380\n",
            "Epoch 60: MSE = 0.3339\n",
            "Epoch 61: MSE = 0.3299\n",
            "Epoch 62: MSE = 0.3259\n",
            "Epoch 63: MSE = 0.3220\n",
            "Epoch 64: MSE = 0.3181\n",
            "Epoch 65: MSE = 0.3143\n",
            "Epoch 66: MSE = 0.3105\n",
            "Epoch 67: MSE = 0.3068\n",
            "Epoch 68: MSE = 0.3031\n",
            "Epoch 69: MSE = 0.2995\n",
            "Epoch 70: MSE = 0.2959\n",
            "Epoch 71: MSE = 0.2923\n",
            "Epoch 72: MSE = 0.2888\n",
            "Epoch 73: MSE = 0.2854\n",
            "Epoch 74: MSE = 0.2820\n",
            "Epoch 75: MSE = 0.2786\n",
            "Epoch 76: MSE = 0.2753\n",
            "Epoch 77: MSE = 0.2720\n",
            "Epoch 78: MSE = 0.2688\n",
            "Epoch 79: MSE = 0.2656\n",
            "Epoch 80: MSE = 0.2624\n",
            "Epoch 81: MSE = 0.2593\n",
            "Epoch 82: MSE = 0.2562\n",
            "Epoch 83: MSE = 0.2532\n",
            "Epoch 84: MSE = 0.2502\n",
            "Epoch 85: MSE = 0.2472\n",
            "Epoch 86: MSE = 0.2443\n",
            "Epoch 87: MSE = 0.2414\n",
            "Epoch 88: MSE = 0.2386\n",
            "Epoch 89: MSE = 0.2357\n",
            "Epoch 90: MSE = 0.2330\n",
            "Epoch 91: MSE = 0.2302\n",
            "Epoch 92: MSE = 0.2275\n",
            "Epoch 93: MSE = 0.2248\n",
            "Epoch 94: MSE = 0.2222\n",
            "Epoch 95: MSE = 0.2196\n",
            "Epoch 96: MSE = 0.2170\n",
            "Epoch 97: MSE = 0.2145\n",
            "Epoch 98: MSE = 0.2119\n",
            "Epoch 99: MSE = 0.2095\n",
            "Epoch 100: MSE = 0.2070\n",
            "Learned weights: [0.259883633193893, 0.30072592290603495, 1.7386750424660056, 1.413471235512617]\n",
            "Learned bias: 3.2963877533361714\n",
            "\n",
            "Training for n=5 features:\n",
            "True weights: [-0.9387737597884769, 0.6787330140640087, 0.9994935789088633, 0.3019466552468697, -0.26582118593998416]\n",
            "True bias: 5\n",
            "Epoch 1: MSE = 23.1701\n",
            "Epoch 2: MSE = 14.4179\n",
            "Epoch 3: MSE = 9.0502\n",
            "Epoch 4: MSE = 5.7565\n",
            "Epoch 5: MSE = 3.7340\n",
            "Epoch 6: MSE = 2.4908\n",
            "Epoch 7: MSE = 1.7256\n",
            "Epoch 8: MSE = 1.2536\n",
            "Epoch 9: MSE = 0.9616\n",
            "Epoch 10: MSE = 0.7802\n",
            "Epoch 11: MSE = 0.6668\n",
            "Epoch 12: MSE = 0.5952\n",
            "Epoch 13: MSE = 0.5493\n",
            "Epoch 14: MSE = 0.5194\n",
            "Epoch 15: MSE = 0.4994\n",
            "Epoch 16: MSE = 0.4855\n",
            "Epoch 17: MSE = 0.4754\n",
            "Epoch 18: MSE = 0.4677\n",
            "Epoch 19: MSE = 0.4615\n",
            "Epoch 20: MSE = 0.4562\n",
            "Epoch 21: MSE = 0.4516\n",
            "Epoch 22: MSE = 0.4475\n",
            "Epoch 23: MSE = 0.4435\n",
            "Epoch 24: MSE = 0.4398\n",
            "Epoch 25: MSE = 0.4362\n",
            "Epoch 26: MSE = 0.4327\n",
            "Epoch 27: MSE = 0.4293\n",
            "Epoch 28: MSE = 0.4260\n",
            "Epoch 29: MSE = 0.4227\n",
            "Epoch 30: MSE = 0.4195\n",
            "Epoch 31: MSE = 0.4163\n",
            "Epoch 32: MSE = 0.4131\n",
            "Epoch 33: MSE = 0.4100\n",
            "Epoch 34: MSE = 0.4069\n",
            "Epoch 35: MSE = 0.4039\n",
            "Epoch 36: MSE = 0.4008\n",
            "Epoch 37: MSE = 0.3978\n",
            "Epoch 38: MSE = 0.3949\n",
            "Epoch 39: MSE = 0.3919\n",
            "Epoch 40: MSE = 0.3890\n",
            "Epoch 41: MSE = 0.3861\n",
            "Epoch 42: MSE = 0.3833\n",
            "Epoch 43: MSE = 0.3805\n",
            "Epoch 44: MSE = 0.3777\n",
            "Epoch 45: MSE = 0.3749\n",
            "Epoch 46: MSE = 0.3721\n",
            "Epoch 47: MSE = 0.3694\n",
            "Epoch 48: MSE = 0.3667\n",
            "Epoch 49: MSE = 0.3640\n",
            "Epoch 50: MSE = 0.3614\n",
            "Epoch 51: MSE = 0.3588\n",
            "Epoch 52: MSE = 0.3561\n",
            "Epoch 53: MSE = 0.3536\n",
            "Epoch 54: MSE = 0.3510\n",
            "Epoch 55: MSE = 0.3485\n",
            "Epoch 56: MSE = 0.3460\n",
            "Epoch 57: MSE = 0.3435\n",
            "Epoch 58: MSE = 0.3410\n",
            "Epoch 59: MSE = 0.3386\n",
            "Epoch 60: MSE = 0.3361\n",
            "Epoch 61: MSE = 0.3337\n",
            "Epoch 62: MSE = 0.3314\n",
            "Epoch 63: MSE = 0.3290\n",
            "Epoch 64: MSE = 0.3267\n",
            "Epoch 65: MSE = 0.3243\n",
            "Epoch 66: MSE = 0.3221\n",
            "Epoch 67: MSE = 0.3198\n",
            "Epoch 68: MSE = 0.3175\n",
            "Epoch 69: MSE = 0.3153\n",
            "Epoch 70: MSE = 0.3131\n",
            "Epoch 71: MSE = 0.3109\n",
            "Epoch 72: MSE = 0.3087\n",
            "Epoch 73: MSE = 0.3065\n",
            "Epoch 74: MSE = 0.3044\n",
            "Epoch 75: MSE = 0.3022\n",
            "Epoch 76: MSE = 0.3001\n",
            "Epoch 77: MSE = 0.2981\n",
            "Epoch 78: MSE = 0.2960\n",
            "Epoch 79: MSE = 0.2939\n",
            "Epoch 80: MSE = 0.2919\n",
            "Epoch 81: MSE = 0.2899\n",
            "Epoch 82: MSE = 0.2879\n",
            "Epoch 83: MSE = 0.2859\n",
            "Epoch 84: MSE = 0.2839\n",
            "Epoch 85: MSE = 0.2820\n",
            "Epoch 86: MSE = 0.2800\n",
            "Epoch 87: MSE = 0.2781\n",
            "Epoch 88: MSE = 0.2762\n",
            "Epoch 89: MSE = 0.2743\n",
            "Epoch 90: MSE = 0.2724\n",
            "Epoch 91: MSE = 0.2706\n",
            "Epoch 92: MSE = 0.2688\n",
            "Epoch 93: MSE = 0.2669\n",
            "Epoch 94: MSE = 0.2651\n",
            "Epoch 95: MSE = 0.2633\n",
            "Epoch 96: MSE = 0.2615\n",
            "Epoch 97: MSE = 0.2598\n",
            "Epoch 98: MSE = 0.2580\n",
            "Epoch 99: MSE = 0.2563\n",
            "Epoch 100: MSE = 0.2546\n",
            "Learned weights: [0.4246771778682557, 1.1824334375681982, 0.996962442987931, 1.2529808162095315, 0.7266381442435756]\n",
            "Learned bias: 2.856451374753264\n"
          ]
        }
      ]
    }
  ]
}